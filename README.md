# ğŸ§  Hadoop + Spark + Jupyter Cluster (Docker Compose)

Este repositorio contÃ©n un **entorno completo de Big Data** baseado en contedores Docker, que integra:
- **Hadoop (HDFS + YARN)**  
- **Spark** (con *History Server* para monitorizar aplicaciÃ³ns)
- **JupyterLab** como interface interactiva de anÃ¡lise e desenvolvemento en PySpark

---

## ğŸ“‚ Estrutura do repositorio

```
.
â”œâ”€â”€ conf/                     # ConfiguraciÃ³n de Hadoop (XML)
â”‚   â”œâ”€â”€ core-site.xml
â”‚   â”œâ”€â”€ hdfs-site.xml
â”‚   â”œâ”€â”€ mapred-site.xml
â”‚   â””â”€â”€ yarn-site.xml
â”‚
â”œâ”€â”€ jupyter/                  # Imaxe e dependencias do contedor Jupyter
â”‚   â”œâ”€â”€ Dockerfile            # ConstrÃºe o notebook con Spark e dependencias Python
â”‚   â””â”€â”€ requirements.txt      # Lista de paquetes Python instalados
â”‚
â”œâ”€â”€ work/                     # Espazo compartido de traballo (ligado ao notebook)
â”‚                             # AquÃ­ podes gardar os notebooks e datos
â”‚
â”œâ”€â”€ start-dfs.sh              # Script de arranque do NameNode (formateo e inicio do HDFS)
â”œâ”€â”€ start-history.sh          # Script de arranque do Spark History Server
â”‚
â”œâ”€â”€ Dockerfile                # Imaxe base con Hadoop + Spark
â”œâ”€â”€ docker-compose.yml        # OrquestraciÃ³n dos contedores
â””â”€â”€ README.md                 # Este documento
```

---

## ğŸš€ Despregue do clÃºster

### 1ï¸âƒ£ ConstruÃ­r as imaxes

ConstrÃºe primeiro a imaxe base con Hadoop + Spark:

```bash
docker build -t adbgonzalez/spark:test-lean -f Dockerfile .
```

Logo a imaxe de Jupyter:

```bash
docker build -t adbgonzalez/spark-notebook:test-lean -f jupyter/Dockerfile jupyter
```

---

### 2ï¸âƒ£ Arrancar o clÃºster

```bash
docker compose up -d
```

Isto lanza os seguintes servizos:

| Servizo | Rol | Portos principais |
|----------|-----|------------------|
| `namenode` | HDFS NameNode | 9870 (UI), 9000 (servizo) |
| `datanode` | HDFS DataNode | 9864 |
| `resourcemanager` | YARN ResourceManager | 8088 |
| `nodemanager` | YARN NodeManager | 8042 |
| `spark-history` | Spark History Server | 18080 |
| `notebook` | JupyterLab con PySpark | 8888 |

---

### 3ï¸âƒ£ Acceso Ã¡s interfaces web

| Servizo | URL | DescriciÃ³n |
|----------|-----|------------|
| **JupyterLab** | [http://localhost:8888](http://localhost:8888) | Entorno de traballo en Python / PySpark |
| **HDFS NameNode** | [http://localhost:9870](http://localhost:9870) | Navegador de ficheiros de HDFS |
| **YARN ResourceManager** | [http://localhost:8088](http://localhost:8088) | Seguimento de tarefas YARN |
| **Spark History Server** | [http://localhost:18080](http://localhost:18080) | HistÃ³rico de aplicaciÃ³ns Spark |

---

## ğŸ§© Uso bÃ¡sico

### ğŸ“˜ 1. Proba rÃ¡pida de Spark dende o notebook

Abre [http://localhost:8888](http://localhost:8888) e crea un novo Notebook Python con este cÃ³digo:

```python
import findspark
findspark.init("/opt/spark")

from pyspark.sql import SparkSession

spark = (
    SparkSession.builder
    .appName("Test-YARN")
    .master("yarn")
    .config("spark.eventLog.enabled", "true")
    .config("spark.eventLog.dir", "hdfs://namenode:9000/spark-logs")
    .getOrCreate()
)

spark.range(1, 1000000).selectExpr("sum(id)").show()
```

O resultado deberÃ­as poder velo tamÃ©n no **Spark History Server** (http://localhost:18080).

---

## âš™ï¸ Directorios e volumes

Os volumes de datos de HDFS estÃ¡n xestionados automaticamente por Docker Compose:

| Volume | Ruta dentro do contedor | Contido |
|---------|--------------------------|----------|
| `namenode_data` | `/home/hadoop/namenode` | Metadatos do NameNode |
| `datanode_data` | `/home/hadoop/datanode` | Bloques de datos do HDFS |
| `./work` | `/home/hadoop/work` | Directorio compartido co host (para notebooks e datos) |

---

## ğŸ§¼ Apagar o clÃºster

```bash
docker compose down
```

Se queres eliminar tamÃ©n os datos persistentes (âš ï¸ borra HDFS!):

```bash
docker compose down -v
```

---

## ğŸ§  Notas adicionais

- O History Server require que o directorio `/spark-logs` exista en HDFS:
  ```bash
  docker compose exec namenode bash -lc "hdfs dfs -mkdir -p /spark-logs && hdfs dfs -chmod 1777 /spark-logs"
  ```
- O notebook xa trae todas as dependencias listadas en `jupyter/requirements.txt`.
- Podes engadir notebooks novos no directorio `work/` e aparecerÃ¡n dentro de JupyterLab.

---

## ğŸ§© CrÃ©dits

ConfiguraciÃ³n adaptada e mantida por **AdriÃ¡n Blanco (CIFP A Carballeira)**  
Baseada en imaxes personalizadas de Hadoop + Spark para docencia e prÃ¡ctica en contornos de Big Data.
